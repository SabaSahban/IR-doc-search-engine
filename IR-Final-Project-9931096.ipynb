{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-10T20:42:37.815296Z",
     "start_time": "2025-01-10T20:42:37.541874Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from hazm import Normalizer, word_tokenize, Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "data": {
      "text/plain": "{'title': 'سجادی :حضور تماشاگران در  لیگ برتر فوتبال تابع نظر فدراسیون  و سازمان لیگ است',\n 'content': '\\nبه گزارش خبرگزاری فارس، سید حمید سجادی در حاشیه مراسم گرامیداشت روز جوان در جمع خبرنگاران در رابطه با عرضه سهام سرخابی\\u200cها در بورس اظهار داشت: منتظر طی روند هستیم و بعدا اطلاع رسانی خواهیم کرد. وی در مورد حضور تماشاگران در مسابقات فوتبال اظهار داشت:\\xa0حضور تماشاگران در\\xa0 لیگ برتر فوتبال تابع نظر فدراسیون ،سازمان لیگ و ستاد ملی مبارزه با کرونا است. انتهای پیام/\\n\\n\\n',\n 'url': 'https://www.farsnews.ir/news/14001224000982/سجادی-حضور-تماشاگران-در-لیگ-برتر-فوتبال-تابع-نظر-فدراسیون-و-سازمان'}"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_docs():\n",
    "    docs = {}\n",
    "    contents = []\n",
    "    urls = []\n",
    "    with open(\"IR_data_news_12k.json\", 'r') as file:\n",
    "        docs = json.load(file)\n",
    "        for key in docs.keys():\n",
    "            idx = str(key)\n",
    "            docs[idx] = {'title': docs[idx]['title'],\n",
    "                         'content': docs[idx]['content'],\n",
    "                         'url': docs[idx]['url'],\n",
    "                         }\n",
    "            contents.append(docs[idx]['content'])\n",
    "    return docs, contents, urls\n",
    "\n",
    "docs, contents, urls = load_docs()\n",
    "docs['1']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-10T20:42:38.213673Z",
     "start_time": "2025-01-10T20:42:37.550462Z"
    }
   },
   "id": "660083e3ccffa505"
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "ABBREVIATIONS_FILE = \"abbreviations.txt\"\n",
    "\n",
    "SPACING_PATTERNS = [\n",
    "    (r\"(\\S)(ها|هاي|هایی|تر|تري|ترین|گر|گري|ام|ات|اش)(\\s|$)\", r\"\\1‌\\2 \"),\n",
    "    (r\"\\b(می|نمی)\\s+(\\S)\", r\"\\1‌\\2\"),\n",
    "]\n",
    "\n",
    "EXTRA_PUNCT_PATTERN = r\"[!<>.,؛،:\\-–_=+(){}\\[\\]…\\\"\\'?؟«»٪%]+\"\n",
    "normalizer = Normalizer()\n",
    "\n",
    "\n",
    "def load_abbreviations(file_path):\n",
    "    \"\"\"Loads abbreviations from the file into a dictionary.\"\"\"\n",
    "    abbreviations = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                entry = eval(line.strip()) \n",
    "                abbreviations.update(entry)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing line: {line.strip()} - {e}\")\n",
    "    return abbreviations\n",
    "\n",
    "\n",
    "def expand_abbreviations(text, abbreviations):\n",
    "    \"\"\"Expands abbreviations using the loaded abbreviations dictionary.\"\"\"\n",
    "    for short_form, expansion in abbreviations.items():\n",
    "        pattern = fr\"\\b{re.escape(short_form)}\\b\"\n",
    "        text = re.sub(pattern, expansion, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def spacing_correction(text):\n",
    "    \"\"\"Applies spacing corrections.\"\"\"\n",
    "    for pattern, repl in SPACING_PATTERNS:\n",
    "        text = re.sub(pattern, repl, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    \"\"\"Removes diacritics and extra punctuation.\"\"\"\n",
    "    text = re.sub(EXTRA_PUNCT_PATTERN, \"\", text) \n",
    "    return text\n",
    "\n",
    "\n",
    "abbreviations = load_abbreviations(ABBREVIATIONS_FILE)\n",
    "\n",
    "def preprocess_single_text(text, expand_abbr=True, preserve_email_id=True, do_spacing_corr=True,\n",
    "                           remove_diacritics=True):\n",
    "    if preserve_email_id:\n",
    "        text = clean_emails(text)\n",
    "   \n",
    "    if expand_abbr:\n",
    "        text = expand_abbreviations(text, abbreviations)\n",
    "\n",
    "    text = normalizer.normalize(text)  \n",
    "\n",
    "    if do_spacing_corr:\n",
    "        text = spacing_correction(text)\n",
    "\n",
    "    if remove_diacritics:\n",
    "        text = remove_punct(text)\n",
    "\n",
    "    tokens = word_tokenize(text)  \n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-10T21:38:04.036321Z",
     "start_time": "2025-01-10T21:38:00.778810Z"
    }
   },
   "id": "b08413092e1dba1d"
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1:\n",
      "Original: لطفاً با ایمیل example@example.com تماس بگیرید.\n",
      "Processed Tokens: ['لطفا', 'با', 'ایمیل', 'EMAILexample', 'at', 'example', 'dot', 'comEMAIL', 'تماس', 'بگیرید']\n",
      "--------------------------------------------------\n",
      "Test 2:\n",
      "Original: پیام از کاربر @user123 در مورد پروژه دریافت شد.\n",
      "Processed Tokens: ['پی\\u200cام', 'از', 'کاربر', '@user', '۱۲۳', 'در', 'مورد', 'پروژه', 'دریافت', 'شد']\n",
      "--------------------------------------------------\n",
      "Test 3:\n",
      "Original: من   می خواهم که نمی روم  به این مکان. همچنین کتاب‌هایم  ترمیم شدند.\n",
      "Processed Tokens: ['من', 'می\\u200cخواهم', 'که', 'نمی\\u200cروم', 'به', 'این', 'مکان', 'همچنین', 'کتاب\\u200cهایم', 'ترمیم', 'شدند']\n",
      "--------------------------------------------------\n",
      "Test 4:\n",
      "Original: كاف و يای عربی به شکل فارسی تبدیل می‌شود. بسم‌الله الرّحمن الرّحيم.\n",
      "Processed Tokens: ['کاف', 'و', 'یای', 'عربی', 'به', 'شکل', 'فارسی', 'تبدیل', 'می\\u200cشود', 'بسم\\u200cالله', 'الرحمن', 'الرحیم']\n",
      "--------------------------------------------------\n",
      "Test 5:\n",
      "Original: إِنَّ اللهَ غَفورٌ رَحيمٌ.\n",
      "Processed Tokens: ['إن', 'الله', 'غفور', 'رحیم']\n",
      "--------------------------------------------------\n",
      "Test 6:\n",
      "Original: سلام! این یک جمله است... یا شاید جمله‌ای دیگر؟!\n",
      "Processed Tokens: ['سلام', 'این', 'یک', 'جمله', 'است', 'یا', 'شاید', 'جمله\\u200cای', 'دیگر']\n",
      "--------------------------------------------------\n",
      "Test 7:\n",
      "Original: شماره تماس 1234567890 به شماره ۱۲۳۴۵۶۷۸۹۰ تبدیل شود.\n",
      "Processed Tokens: ['شماره', 'تماس', '۱۲۳۴۵۶۷۸۹۰', 'به', 'شماره', '۱۲۳۴۵۶۷۸۹۰', 'تبدیل', 'شود']\n",
      "--------------------------------------------------\n",
      "Test 8:\n",
      "Original: در سال‌های گذشته کتاب‌های زیادی خواندم. سال‌ها می‌گذرد و کتاب‌ها تکرار می‌شوند.\n",
      "Processed Tokens: ['در', 'سال\\u200cهای', 'گذشته', 'کتاب\\u200cهای', 'زیادی', 'خواندم', 'سال\\u200c\\u200cها', 'می\\u200cگذرد', 'و', 'کتاب\\u200c\\u200cها', 'تکرار', 'می\\u200cشوند']\n",
      "--------------------------------------------------\n",
      "Test 9:\n",
      "Original: بِسْمِ اللَّهِ الرَّحْمَٰنِ الرَّحِيمِ و ﷽.\n",
      "Processed Tokens: ['بسم\\u200cالله', 'الرحمن', 'الرحیم', 'و', 'بسم', 'الله', 'الرحمن', 'الرحیم']\n",
      "--------------------------------------------------\n",
      "Test 10:\n",
      "Original: آیا !؟ و ... را باید حذف کنیم؟\n",
      "Processed Tokens: ['آیا', 'و', 'را', 'باید', 'حذف', 'کنیم']\n",
      "--------------------------------------------------\n",
      "Test 11:\n",
      "Original: نمی‌شود \"می‌خواهم\" را به دو توکن تبدیل کرد.\n",
      "Processed Tokens: ['نمی\\u200cشود', 'می\\u200cخواهم', 'را', 'به', 'دو', 'توکن', 'تبدیل', 'کرد']\n",
      "--------------------------------------------------\n",
      "Test 12:\n",
      "Original: Contact me at user@example.edu. شماره من ۱۲۳ است.ه.ش.\n",
      "Processed Tokens: ['Contact', 'me', 'at', 'EMAILuser', 'at', 'example', 'dot', 'eduEMAIL', 'شماره', 'من', '۱۲۳', 'است', 'هجری', 'شمسی']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def test_normalization():\n",
    "    \"\"\"Tests the normalization pipeline with various sentences.\"\"\"\n",
    "    test_sentences = [\n",
    "        \"لطفاً با ایمیل example@example.com تماس بگیرید.\",\n",
    "        \"پیام از کاربر @user123 در مورد پروژه دریافت شد.\",\n",
    "        \"من   می خواهم که نمی روم  به این مکان. همچنین کتاب‌هایم  ترمیم شدند.\",\n",
    "        \"كاف و يای عربی به شکل فارسی تبدیل می‌شود. بسم‌الله الرّحمن الرّحيم.\",\n",
    "        \"إِنَّ اللهَ غَفورٌ رَحيمٌ.\",\n",
    "        \"سلام! این یک جمله است... یا شاید جمله‌ای دیگر؟!\",\n",
    "        \"شماره تماس 1234567890 به شماره ۱۲۳۴۵۶۷۸۹۰ تبدیل شود.\",\n",
    "        \"در سال‌های گذشته کتاب‌های زیادی خواندم. سال‌ها می‌گذرد و کتاب‌ها تکرار می‌شوند.\",\n",
    "        \"بِسْمِ اللَّهِ الرَّحْمَٰنِ الرَّحِيمِ و ﷽.\",\n",
    "        \"آیا !؟ و ... را باید حذف کنیم؟\",\n",
    "        \"نمی‌شود \\\"می‌خواهم\\\" را به دو توکن تبدیل کرد.\",\n",
    "        \"Contact me at user@example.edu. شماره من ۱۲۳ است.\"\n",
    "        \"ه.ش.\"\n",
    "    ]\n",
    "    \n",
    "    for i, sentence in enumerate(test_sentences, 1):\n",
    "        print(f\"Test {i}:\")\n",
    "        print(\"Original:\", sentence)\n",
    "        tokens = preprocess_single_text(sentence)\n",
    "        print(\"Processed Tokens:\", tokens)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the test function\n",
    "test_normalization()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-10T21:38:04.646881Z",
     "start_time": "2025-01-10T21:38:04.629393Z"
    }
   },
   "id": "edaca9ab3e418760"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "stemmer = Stemmer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    \"\"\"Applies stemming to a list of tokens.\"\"\"\n",
    "    return [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "def compute_top_k_frequent(tokens, k):\n",
    "    \"\"\"Computes the top-K most frequent tokens along with their frequencies.\"\"\"\n",
    "    token_counts = Counter(tokens)\n",
    "    sorted_tokens = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    return dict(sorted_tokens[:k])\n",
    "\n",
    "def simple_preprocess(content, top_k_tokens=None):\n",
    "    \"\"\"\n",
    "    Simplified preprocessing pipeline for a single document:\n",
    "      - Normalize and tokenize the content.\n",
    "      - Stem tokens.\n",
    "      - Optionally remove tokens present in `top_k_tokens`.\n",
    "    \"\"\"\n",
    "    # Step 1: Normalize and tokenize the content\n",
    "    tokens = preprocess_single_text(content)  \n",
    "\n",
    "    # Step 2: Stem tokens\n",
    "    tokens = stem_tokens(tokens) \n",
    "\n",
    "    # Step 3: Remove top-K frequent tokens if provided\n",
    "    if top_k_tokens:\n",
    "        tokens = [token for token in tokens if token not in top_k_tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def preprocess_all_docs(docs, top_k=50):\n",
    "    \"\"\"\n",
    "    Preprocesses all documents while returning the same structure as required:\n",
    "      - Normalizes, tokenizes, and stems content for each document.\n",
    "      - Computes top-K frequent tokens across all documents.\n",
    "      - Removes top-K frequent tokens from each document's content.\n",
    "    \"\"\"\n",
    "    combined_tokens = []\n",
    "    all_tokens = {}\n",
    "\n",
    "    for doc_id, doc_data in docs.items():\n",
    "        content = doc_data['content']\n",
    "        tokens = simple_preprocess(content) \n",
    "        all_tokens[doc_id] = tokens\n",
    "        combined_tokens.extend(tokens)\n",
    "\n",
    "    top_k_tokens_with_counts = compute_top_k_frequent(combined_tokens, top_k)\n",
    "    top_k_tokens = set(top_k_tokens_with_counts.keys())  # Extract tokens only\n",
    "\n",
    "    for doc_id in all_tokens:\n",
    "        filtered_tokens = [token for token in all_tokens[doc_id] if token not in top_k_tokens]\n",
    "        docs[doc_id]['content'] = filtered_tokens\n",
    "\n",
    "    return docs, top_k_tokens_with_counts"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-10T20:42:39.372682Z",
     "start_time": "2025-01-10T20:42:39.367363Z"
    }
   },
   "id": "4f5b3ea8c4ce8578"
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-K Frequent Tokens:\n",
      "Token: و, Count: 234908\n",
      "Token: در, Count: 165135\n",
      "Token: به, Count: 136045\n",
      "Token: از, Count: 92977\n",
      "Token: این, Count: 83094\n",
      "Token: که, Count: 75480\n",
      "Token: با, Count: 69233\n",
      "Token: را, Count: 68677\n",
      "Token: اس, Count: 48513\n",
      "Token: برا, Count: 31029\n",
      "Token: کرد, Count: 26941\n",
      "Token: آن, Count: 24578\n",
      "Token: ه, Count: 24415\n",
      "Token: یک, Count: 22864\n",
      "Token: کشور, Count: 22345\n",
      "Token: ت, Count: 22173\n",
      "Token: ما, Count: 19785\n",
      "Token: خود, Count: 18861\n",
      "Token: بر, Count: 18807\n",
      "Token: شد, Count: 17270\n",
      "\n",
      "Processed Document Tokens:\n",
      "Doc 0 final tokens: ['گزار', 'خبرگزار', 'فارس', 'کنفدراسیون', 'فوتبال', 'آسیا', 'AFC', 'نامه', 'رسم', 'فدراسیون', 'فوتبال', 'ایر', 'باشگاه', 'گیت', 'پسند', 'ز', 'قرعه\\u200cکش', 'ج', 'باشگاه', 'فوتسال', 'آسیا', 'رسما', 'اعل', 'اساس', '۲۵', 'فروردین\\u200cماه', '۱۴۰۱', 'مراس', 'قرعه\\u200cکش', 'ج', 'باشگاه', 'فوتسال', 'آسیا', 'مالز', 'برگزار', 'می\\u200cشود', 'باشگاه', 'گیت', 'پسند', 'بعنو', 'قهر', 'فوتسال', 'ایر', 'سال', '۱۴۰۰', 'مسابق', 'راه', 'پیدا', 'کرده_اس', 'پ', 'گیت', 'پسند', 'تجربه', '۳', 'دوره', 'حضور', 'ج', 'باشگاه', 'فوتسال', 'آسیا', 'داشته', 'هر', 'سه', 'دوره', 'فینال', 'مسابق', 'راه', 'پیدا', 'کرده', 'عنو', 'قهرمان', 'دو', 'مق', 'دوم', 'بدس', 'آورده_اس', 'انت', 'پی', '/']\n",
      "Doc 1 final tokens: ['گزار', 'خبرگزار', 'فارس', 'سید', 'حمید', 'سجاد', 'حاشیه', 'مراس', 'گرامیدا', 'روز', 'جو', 'جمع', 'خبرنگار', 'رابطه', 'عرضه', 'سه', 'سرخابی\\u200c', 'بورس', 'اظهار', 'دا', 'منتظر', 'ط', 'روند', 'هست', 'بعدا', 'اطلاع\\u200cرسان', 'خواهیم_کرد', 'مورد', 'حضور', 'تماشاگر', 'مسابق', 'فوتبال', 'اظهار', 'دا', 'حضور', 'تماشاگر', 'لیگ', 'فوتبال', 'تابع', 'نظر', 'فدراسیون', 'ساز', 'لیگ', 'ستاد', 'مل', 'مبارزه', 'کرونا', 'انت', 'پی', '/']\n",
      "Doc 2 final tokens: ['گزار', 'خبرگزار', 'فارس', 'نشس', 'خبر', 'پ', 'مسابقه', 'سرمرب', 'دو', 'پرسپولیس', 'استقلال', 'هفته', 'بیس', 'سو', 'لیگ', 'مدیر', 'ساز', 'لیگ', 'هماهنگ', 'باشگاه', 'میزب', 'پرسپولیس', 'شرح', 'زیر', 'برگزار', 'می\\u200cشود', 'چهارشنبه', '۲۵', 'اسفند', 'ساع', '۱۵', 'فرهاد', 'مجید', 'سرمرب', 'استقلال', 'ساز', 'لیگ', 'فوتبال', 'ایر', 'ساع', '۱۳۳۰', 'یح', 'گل', 'محمد', 'سرمرب', 'پرسپولیس', 'ورزشگاه', 'شهید', 'کاظم', 'مسابقه', 'دو', 'روز', 'پنجشنبه', 'ورزشگاه', 'آزاد', 'برگزار', 'می\\u200cشود', 'انت', 'پی', '/']\n",
      "Doc 3 final tokens: ['گزار', 'خبرگزار', 'فارس', 'سید', 'رضا', 'صالح', 'امیر', 'رئیس', 'کمیته', 'مل', 'المپیک', 'دیدار', 'سید', 'میرشاد', 'ماجد', 'سرپرس', 'فدراسیون', 'فوتبال', 'آمادگ', 'کمیته', 'منظور', 'حما', 'همه\\u200cجانبه', 'المپیک', 'ایر', 'ضرور', 'پیش\\u200cبین', 'برنامه', 'آماده', 'ساز', 'مناسب', 'تاکید', 'گف', 'ضمن', 'آرزو', 'موفق', 'آقا', 'ماجد', 'عنو', 'چهره', 'اخلاق', 'فوتبال', 'ورز', 'تم', 'تلا', 'خواهد_بود', 'تا', 'امید', 'المپیک', 'محیط', 'آر', 'بدون', 'دغدغه', 'آماده', 'حضور', 'بازی', 'آسیا', 'شود', 'ضمن', 'اینکه', 'پ', 'اعل', 'دخالت', 'امور', 'فن', 'تیم\\u200c', 'نداشته', 'نخواهیم_دا', 'سرپرس', 'فدراسیون', 'فوتبال', 'نیز', 'تاکید', 'ضرور', 'تعامل', 'همکار', 'همه\\u200cجانبه', 'خصوص', 'برنامه\\u200cریز', 'آماده', 'ساز', 'مناسب', 'المپیک', 'گف', 'فوتبال', 'زیر', '۲۳', 'سال', 'ایر', 'حضور', 'مهد', 'مهدو', 'کیا', 'اعضا', 'کادر', 'فن', 'همچنین', 'ظرف', 'بسیار', 'خوب', 'موفق', 'دوره', 'برخوردار', 'هماهنگ', 'همکار', 'نزدیک', 'کمیته', 'مل', 'المپیک', 'فدراسیون', 'فوتبال', 'راستا', 'حما', 'تامین', 'خواسته', 'مورد', 'نظر', 'اعضا', 'کادر', 'فن', 'می\\u200cتواند', 'نتایج', 'ارزشمند', 'همراه', 'داشته_باشد', 'امیدوار', 'دوره', 'شاهد', 'موفق', 'المپیک', 'کسب', 'نتایج', '', 'جایگاه', 'فوتبال', 'مل', 'ایر', 'باش', 'نشس', 'حضور', 'نصرالله', 'سجاد', 'مشاور', 'رئیس', 'کمیته', 'پ', 'فخر', 'سرپرس', 'کارو', 'ورزش', 'اعزام', 'ایر', 'بازی', 'آسیا', '۲۰۲۲', 'هانگژو', 'برگزار', 'هماهنگ', 'لاز', 'خصوص', 'همکار', 'دو', 'جانبه', 'فدراسیون', 'فوتبال', 'کمیته', 'مل', 'المپیک', 'خصوص', 'برنامه\\u200c', 'اردو', 'آماده', 'ساز', 'آت', 'المپیک', 'صور', 'پذیرف', 'انت', 'پی', '/']\n",
      "Doc 4 final tokens: ['گزار', 'خبرنگار', 'ورزش', 'خبرگزار', 'فارس', 'نخستین', 'باز', 'روز', 'دو', 'مرحله', 'پلی\\u200cآف', 'لیگ', 'بسکتبال', 'تیم', 'شهردار', 'گرگ', 'اکسون', 'مقابل', 'مید', 'رفتند', 'گرگانی\\u200c', 'موفق', 'شدند', '۸۰', '۷۱', 'برتر', 'برسند', 'مسابقه', 'پر', 'پت', 'کسب', '۲۶', 'امتیاز', '۶', 'ریباند', 'پرامتیاز', 'بازیکن', 'گرگ', 'مید', 'بود', 'دی\\u200cگر', 'باز', 'برگزار', 'شده', 'ذوب\\u200cآهن', 'صدرنشین', 'پلی\\u200cآف', 'رسیده_بود', 'دیدار', 'صنایع', 'هرمزگ', '۷۳', '۷۰', 'برتر', 'رسید', 'انت', 'پی', '/']\n"
     ]
    }
   ],
   "source": [
    "docs, contents, _ = load_docs()\n",
    "\n",
    "pre_processed_docs, top_k_tokens_with_counts = preprocess_all_docs(docs, top_k=20)\n",
    "\n",
    "print(\"\\nTop-K Frequent Tokens:\")\n",
    "for token, count in top_k_tokens_with_counts.items():\n",
    "    print(f\"Token: {token}, Count: {count}\")\n",
    "\n",
    "print(\"\\nProcessed Document Tokens:\")\n",
    "for doc_id, doc_data in list(pre_processed_docs.items())[:5]: \n",
    "    print(f\"Doc {doc_id} final tokens:\", doc_data['content'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-10T20:44:15.844601Z",
     "start_time": "2025-01-10T20:42:39.371431Z"
    }
   },
   "id": "880af580a11a5962"
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "data": {
      "text/plain": "{'title': 'سجادی :حضور تماشاگران در  لیگ برتر فوتبال تابع نظر فدراسیون  و سازمان لیگ است',\n 'content': ['گزار',\n  'خبرگزار',\n  'فارس',\n  'سید',\n  'حمید',\n  'سجاد',\n  'حاشیه',\n  'مراس',\n  'گرامیدا',\n  'روز',\n  'جو',\n  'جمع',\n  'خبرنگار',\n  'رابطه',\n  'عرضه',\n  'سه',\n  'سرخابی\\u200c',\n  'بورس',\n  'اظهار',\n  'دا',\n  'منتظر',\n  'ط',\n  'روند',\n  'هست',\n  'بعدا',\n  'اطلاع\\u200cرسان',\n  'خواهیم_کرد',\n  'مورد',\n  'حضور',\n  'تماشاگر',\n  'مسابق',\n  'فوتبال',\n  'اظهار',\n  'دا',\n  'حضور',\n  'تماشاگر',\n  'لیگ',\n  'فوتبال',\n  'تابع',\n  'نظر',\n  'فدراسیون',\n  'ساز',\n  'لیگ',\n  'ستاد',\n  'مل',\n  'مبارزه',\n  'کرونا',\n  'انت',\n  'پی',\n  '/'],\n 'url': 'https://www.farsnews.ir/news/14001224000982/سجادی-حضور-تماشاگران-در-لیگ-برتر-فوتبال-تابع-نظر-فدراسیون-و-سازمان'}"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_processed_docs['1']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-10T20:44:15.850387Z",
     "start_time": "2025-01-10T20:44:15.846623Z"
    }
   },
   "id": "8279efe17e0cfae7"
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "def _add_new_token_posting(token_dict, token, doc_id, position):\n",
    "    \"\"\"\n",
    "    Initialize a new posting entry for a token that has not yet been seen.\n",
    "    \"\"\"\n",
    "    token_dict[token] = {\n",
    "        'frequency': 1,\n",
    "        'docs': {\n",
    "            doc_id: {\n",
    "                'positions': [position],\n",
    "                'number_of_token': 1\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "def _update_existing_token_posting(token_dict, token, doc_id, position):\n",
    "    \"\"\"\n",
    "    Update an existing token entry with a new position in an existing or new document.\n",
    "    \"\"\"\n",
    "    token_dict[token]['frequency'] += 1\n",
    "    \n",
    "    if doc_id in token_dict[token]['docs']:\n",
    "        token_dict[token]['docs'][doc_id]['positions'].append(position)\n",
    "        token_dict[token]['docs'][doc_id]['number_of_token'] += 1\n",
    "    else:\n",
    "        token_dict[token]['docs'][doc_id] = {\n",
    "            'positions': [position],\n",
    "            'number_of_token': 1\n",
    "        }\n",
    "\n",
    "def _build_postings_dict(Docs):\n",
    "    \"\"\"\n",
    "    Build a postings dictionary from the input Docs, collecting token frequency\n",
    "    and positions per document.\n",
    "    \"\"\"\n",
    "    token_dict = {}\n",
    "    for doc_id, doc_content in Docs.items():\n",
    "        for position, token in enumerate(doc_content['content']):\n",
    "            if token in token_dict:\n",
    "                _update_existing_token_posting(token_dict, token, doc_id, position)\n",
    "            else:\n",
    "                _add_new_token_posting(token_dict, token, doc_id, position)\n",
    "    return token_dict\n",
    "\n",
    "def _calculate_tf_idf(token_dict, total_docs):\n",
    "    \"\"\"\n",
    "    Calculate TF-IDF for each token in each document.\n",
    "    \"\"\"\n",
    "    for term, term_data in token_dict.items():\n",
    "        term_docs = term_data['docs']\n",
    "        n_t = len(term_docs)  # Number of documents containing this term\n",
    "\n",
    "        for doc_id, doc_info in term_docs.items():\n",
    "            tf = doc_info['number_of_token']\n",
    "            # TF-IDF = log10(N / n_t) * (1 + log10(tf))\n",
    "            tf_idf_value = (np.log10(total_docs / n_t)) * (1 + np.log10(tf))\n",
    "            doc_info['tf_idf'] = tf_idf_value\n",
    "\n",
    "def _build_champions_list_and_docs_vectors(token_dict, champ_len):\n",
    "    \"\"\"\n",
    "    Create the champions list for each term by sorting documents based on number_of_token,\n",
    "    and build the docs_vectors structure for future usage.\n",
    "    \"\"\"\n",
    "    docs_vectors = {}\n",
    "\n",
    "    for term, term_data in token_dict.items():\n",
    "        term_docs = term_data['docs']\n",
    "\n",
    "        # Sort documents by number_of_token (descending) for the champion list\n",
    "        sorted_term_docs = sorted(\n",
    "            term_docs,\n",
    "            key=lambda d: term_docs[d]['number_of_token'],\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        # Build champion list (take top `champ_len` if needed)\n",
    "        champions_list = {}\n",
    "        for doc_id in sorted_term_docs:\n",
    "            champions_list[doc_id] = {\n",
    "                'number_of_token': term_docs[doc_id]['number_of_token'],\n",
    "                'tf_idf': term_docs[doc_id]['tf_idf']\n",
    "            }\n",
    "\n",
    "        if champ_len < len(term_docs):\n",
    "            champions_list = dict(list(champions_list.items())[:champ_len])\n",
    "\n",
    "        token_dict[term]['champions_list'] = champions_list\n",
    "\n",
    "        # Populate docs_vectors\n",
    "        for doc_id, doc_info in term_docs.items():\n",
    "            if doc_id not in docs_vectors:\n",
    "                docs_vectors[doc_id] = {}\n",
    "            docs_vectors[doc_id][term] = {\n",
    "                'tf_idf': doc_info['tf_idf'],\n",
    "                'tf': doc_info['number_of_token']\n",
    "            }\n",
    "\n",
    "    return docs_vectors\n",
    "\n",
    "def Postings_List(Docs, champ_len):\n",
    "    \"\"\"\n",
    "    Orchestrates the creation of the postings list (token_dict) and docs_vectors.\n",
    "    Steps:\n",
    "      1) Build an initial postings dictionary with token frequencies/positions.\n",
    "      2) Compute TF-IDF for each token in each document.\n",
    "      3) Build the champions lists and docs_vectors.\n",
    "    \"\"\"\n",
    "    # Step 1: Build the core postings dictionary\n",
    "    token_dict = _build_postings_dict(Docs)\n",
    "\n",
    "    # Step 2: Calculate TF-IDF\n",
    "    total_docs = len(Docs)\n",
    "    _calculate_tf_idf(token_dict, total_docs)\n",
    "\n",
    "    # Step 3: Create champions lists and docs_vectors\n",
    "    docs_vectors = _build_champions_list_and_docs_vectors(token_dict, champ_len)\n",
    "\n",
    "    return token_dict, docs_vectors\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-10T20:44:15.864625Z",
     "start_time": "2025-01-10T20:44:15.849699Z"
    }
   },
   "id": "9641ba7139e97bb2"
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "dictionary, docs_vectors = Postings_List(pre_processed_docs, 20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-10T20:44:29.400584Z",
     "start_time": "2025-01-10T20:44:15.935713Z"
    }
   },
   "id": "390b1d1f8d73c4b4"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "{'گزار': {'tf_idf': 0.06822999816007738, 'tf': 1},\n 'خبرگزار': {'tf_idf': 0.006888013253462664, 'tf': 1},\n 'مراس': {'tf_idf': 1.3082797702727251, 'tf': 1},\n 'برگزار': {'tf_idf': 0.7236736350395699, 'tf': 2},\n 'می\\u200cشود': {'tf_idf': 0.7264740996744733, 'tf': 3},\n 'عنو': {'tf_idf': 0.49503246937512, 'tf': 1},\n 'مق': {'tf_idf': 1.1250099265899205, 'tf': 1},\n 'انت': {'tf_idf': 0.004435995384070603, 'tf': 1},\n 'پی': {'tf_idf': 0.003000072495857743, 'tf': 1},\n '/': {'tf_idf': 0.004364086371255635, 'tf': 1},\n 'ساز': {'tf_idf': 0.7613260376849612, 'tf': 1},\n 'مل': {'tf_idf': 0.486329764964978, 'tf': 1},\n 'سو': {'tf_idf': 0.6778518952477012, 'tf': 1},\n 'زیر': {'tf_idf': 0.8060803276103631, 'tf': 1},\n 'برنامه': {'tf_idf': 0.7438109781030208, 'tf': 1},\n 'ورز': {'tf_idf': 1.1212293196304566, 'tf': 1},\n 'امور': {'tf_idf': 1.006526552989648, 'tf': 1},\n 'رقاب': {'tf_idf': 1.371263662807911, 'tf': 1},\n 'نه': {'tf_idf': 0.9870957429704109, 'tf': 1},\n 'میزبان': {'tf_idf': 1.4024838899048566, 'tf': 1},\n 'دار': {'tf_idf': 0.683825778736454, 'tf': 1},\n 'نیرو': {'tf_idf': 1.2502244555234152, 'tf': 2},\n 'جوان': {'tf_idf': 1.053007265169419, 'tf': 1},\n 'ع': {'tf_idf': 1.3282763986889787, 'tf': 1},\n 'خانواده': {'tf_idf': 1.2213270460152408, 'tf': 1},\n 'دفاع': {'tf_idf': 0.9629060397136366, 'tf': 1},\n 'همین': {'tf_idf': 0.6297613912326112, 'tf': 1},\n 'یکشنبه': {'tf_idf': 1.2079092251551622, 'tf': 1},\n 'اقد': {'tf_idf': 0.9390639128625823, 'tf': 1},\n 'سطح': {'tf_idf': 1.4680109144106421, 'tf': 2},\n 'معرف': {'tf_idf': 1.1921149579719303, 'tf': 1},\n 'تفکر': {'tf_idf': 1.6486804578359808, 'tf': 1},\n 'ترب': {'tf_idf': 1.56660302688065, 'tf': 1},\n 'فردا': {'tf_idf': 1.1297824414511655, 'tf': 1},\n 'سپاه': {'tf_idf': 1.4140446589452362, 'tf': 2},\n 'سرباز': {'tf_idf': 3.411576346714269, 'tf': 12},\n 'حضر': {'tf_idf': 1.2610049028885455, 'tf': 1},\n 'علی\\u200cاکبر': {'tf_idf': 1.9045874327115961, 'tf': 1},\n 'خواهند_شد': {'tf_idf': 1.853434910264215, 'tf': 1},\n 'جشنواره': {'tf_idf': 2.9666750093650878, 'tf': 2},\n 'امن': {'tf_idf': 1.2213270460152408, 'tf': 1},\n 'بدن': {'tf_idf': 1.7265955383164808, 'tf': 1},\n 'برجسته': {'tf_idf': 1.9074540733631993, 'tf': 1},\n 'محور': {'tf_idf': 1.2788959925875154, 'tf': 1},\n 'فرهنگ': {'tf_idf': 0.9940613210272481, 'tf': 1},\n 'خلاق': {'tf_idf': 2.127389628335275, 'tf': 1},\n 'نخبه': {'tf_idf': 2.8923304004626003, 'tf': 2},\n 'ذکر': {'tf_idf': 1.5812810423364627, 'tf': 1},\n 'پذیر': {'tf_idf': 1.6833104994805508, 'tf': 1},\n 'مسول': {'tf_idf': 2.9103397616006874, 'tf': 1},\n 'دین': {'tf_idf': 1.4051897832807816, 'tf': 1},\n 'چهاردهمین': {'tf_idf': 2.706219778944763, 'tf': 1},\n 'اقتدار': {'tf_idf': 1.592276426637926, 'tf': 1},\n 'شا': {'tf_idf': 2.2475819299191135, 'tf': 1},\n 'مسلح': {'tf_idf': 1.5679170807784812, 'tf': 1},\n 'هنر': {'tf_idf': 1.787577944246662, 'tf': 1},\n 'بهمن\\u200cماه': {'tf_idf': 1.6180836902442115, 'tf': 1},\n 'بسیج': {'tf_idf': 2.0689879097801427, 'tf': 3},\n 'تجلیل': {'tf_idf': 2.02197303142945, 'tf': 1},\n 'پاسدا': {'tf_idf': 2.5066474240395586, 'tf': 1},\n 'ابتکار': {'tf_idf': 2.007249774608744, 'tf': 1},\n 'فداکار': {'tf_idf': 2.0693976813575885, 'tf': 1},\n 'وظیفه\\u200cشناس': {'tf_idf': 3.132188511217044, 'tf': 1},\n 'تبلیغات': {'tf_idf': 2.172617168272652, 'tf': 1},\n 'بصیر': {'tf_idf': 1.8076774197035397, 'tf': 1},\n 'ایثارگر': {'tf_idf': 1.9965259092169707, 'tf': 1},\n 'فناور': {'tf_idf': 1.4854581249696206, 'tf': 1},\n 'سازندگ': {'tf_idf': 2.025733180302757, 'tf': 1},\n 'مستضعفین': {'tf_idf': 2.2056174283755774, 'tf': 1},\n 'پنج\\u200cگانه': {'tf_idf': 3.241332980642112, 'tf': 1}}"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_vectors['8535']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-10T20:44:29.409688Z",
     "start_time": "2025-01-10T20:44:29.399456Z"
    }
   },
   "id": "9cea4d7dc4bae15a"
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "{'7435': {'number_of_token': 64, 'tf_idf': 0.0059070807944042346},\n '11697': {'number_of_token': 36, 'tf_idf': 0.0053810823065394255},\n '163': {'number_of_token': 35, 'tf_idf': 0.005355328462095761},\n '6404': {'number_of_token': 35, 'tf_idf': 0.005355328462095761},\n '1322': {'number_of_token': 33, 'tf_idf': 0.00530153641698586},\n '1633': {'number_of_token': 31, 'tf_idf': 0.00524438024289291},\n '6755': {'number_of_token': 27, 'tf_idf': 0.00511808306260702},\n '7584': {'number_of_token': 26, 'tf_idf': 0.005083580816288167},\n '821': {'number_of_token': 25, 'tf_idf': 0.005047725200337222},\n '2388': {'number_of_token': 24, 'tf_idf': 0.0050104056913280555},\n '7744': {'number_of_token': 24, 'tf_idf': 0.0050104056913280555},\n '525': {'number_of_token': 19, 'tf_idf': 0.00479683475620343},\n '8680': {'number_of_token': 16, 'tf_idf': 0.004639729076116687},\n '10183': {'number_of_token': 16, 'tf_idf': 0.004639729076116687},\n '10025': {'number_of_token': 15, 'tf_idf': 0.00458072789429455},\n '2831': {'number_of_token': 14, 'tf_idf': 0.00451765454084172},\n '8897': {'number_of_token': 14, 'tf_idf': 0.00451765454084172},\n '9687': {'number_of_token': 14, 'tf_idf': 0.00451765454084172},\n '1684': {'number_of_token': 13, 'tf_idf': 0.004449904957144393},\n '2098': {'number_of_token': 13, 'tf_idf': 0.004449904957144393}}"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['فارس']['champions_list']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-10T20:44:29.434926Z",
     "start_time": "2025-01-10T20:44:29.408190Z"
    }
   },
   "id": "78ec1d58f6254401"
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "data": {
      "text/plain": "{'frequency': 3,\n 'docs': {'325': {'positions': [52],\n   'number_of_token': 1,\n   'tf_idf': 3.7854010249923875},\n  '4430': {'positions': [272, 383],\n   'number_of_token': 2,\n   'tf_idf': 4.924920279132276}},\n 'champions_list': {'4430': {'number_of_token': 2,\n   'tf_idf': 4.924920279132276},\n  '325': {'number_of_token': 1, 'tf_idf': 3.7854010249923875}}}"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary['استرا']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-10T22:12:19.426227Z",
     "start_time": "2025-01-10T22:12:19.392228Z"
    }
   },
   "id": "732987798d7b8b84"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "data": {
      "text/plain": "48166"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-10T20:44:29.559987Z",
     "start_time": "2025-01-10T20:44:29.536903Z"
    }
   },
   "id": "d06340cee6ec0b4a"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "half_length = len(dictionary) // 2\n",
    "first_half = {key: dictionary[key] for key in list(dictionary.keys())[:half_length]}\n",
    "second_half = {key: dictionary[key] for key in list(dictionary.keys())[half_length:]}\n",
    "\n",
    "with open('first_half.json', \"w\", encoding=\"utf-8\") as first_file:\n",
    "    json.dump(first_half, first_file, indent=4)\n",
    "\n",
    "with open('second_half.json', \"w\", encoding=\"utf-8\") as second_file:\n",
    "    json.dump(second_half, second_file, indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-10T20:44:51.844522Z",
     "start_time": "2025-01-10T20:44:30.607400Z"
    }
   },
   "id": "da0b1362554f134a"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "def calculate_tf_idf(f_td, N, n_t):\n",
    "    tf = 1 + np.log10(f_td)\n",
    "    idf = np.log10(N / n_t)\n",
    "    return tf * idf\n",
    "\n",
    "\n",
    "def vector_length(vector_dict):\n",
    "    length = math.sqrt(sum(tf_idf_value['tf_idf'] ** 2 for tf_idf_value in vector_dict.values()))\n",
    "    return length\n",
    "\n",
    "def get_query_tokens(query):\n",
    "    \"\"\"\n",
    "    Preprocesses the query into tokens.\n",
    "    Returns a list of tokens.\n",
    "    \"\"\"\n",
    "    return simple_preprocess(query)\n",
    "\n",
    "def get_query_tokens_count(query_tokens):\n",
    "    \"\"\"\n",
    "    Counts how many times each token appears in the query.\n",
    "    Returns a dictionary of token -> frequency.\n",
    "    \"\"\"\n",
    "    return dict(collections.Counter(query_tokens))\n",
    "\n",
    "def compute_query_weight(term, query_tokens_count, dictionary, champion_list, total_number_of_docs):\n",
    "    \"\"\"\n",
    "    Computes the TF-IDF weight of a query term.\n",
    "    Returns w_tq (the query-term weight) and the term_docs to iterate over.\n",
    "    \"\"\"\n",
    "    if term not in dictionary:\n",
    "        return 0, {}\n",
    "\n",
    "    # Retrieve the appropriate document list (champion list or full list)\n",
    "    term_docs = (dictionary[term]['champions_list'] \n",
    "                 if champion_list \n",
    "                 else dictionary[term]['docs'])\n",
    "\n",
    "    w_tq = calculate_tf_idf(query_tokens_count[term],\n",
    "                            total_number_of_docs,\n",
    "                            len(term_docs))\n",
    "    return w_tq, term_docs\n",
    "\n",
    "def update_doc_scores(term_docs, w_tq, cosine_scores, jaccard_scores):\n",
    "    \"\"\"\n",
    "    Updates the cosine and jaccard scores for each document that contains the term.\n",
    "    \"\"\"\n",
    "    for doc in term_docs:\n",
    "        w_td = term_docs[doc]['tf_idf']\n",
    "        doc_id = int(doc)\n",
    "\n",
    "        # Update cosines similarity\n",
    "        if doc_id in cosine_scores:\n",
    "            cosine_scores[doc_id] += w_td * w_tq\n",
    "            jaccard_scores[doc_id] += 1\n",
    "        else:\n",
    "            cosine_scores[doc_id] = w_td * w_tq\n",
    "            jaccard_scores[doc_id] = 1\n",
    "\n",
    "def finalize_cosine_scores(cosine_scores):\n",
    "    \"\"\"\n",
    "    Divides each document's cosine score by its vector length.\n",
    "    \"\"\"\n",
    "    for doc_number in cosine_scores:\n",
    "        cosine_scores[doc_number] /= vector_length(docs_vectors[str(doc_number)])\n",
    "\n",
    "def finalize_jaccard_scores(jaccard_scores, query_terms_num):\n",
    "    \"\"\"\n",
    "    Calculates jaccard score for each document based on intersection/union.\n",
    "    \"\"\"\n",
    "    for doc_number in jaccard_scores:\n",
    "        doc_length = len(pre_processed_docs[str(doc_number)]['content'])\n",
    "        intersection = jaccard_scores[doc_number]\n",
    "        jaccard_scores[doc_number] = intersection / (doc_length + query_terms_num - intersection)\n",
    "\n",
    "def sort_scores(scores_dict):\n",
    "    \"\"\"\n",
    "    Sorts scores in descending order by value.\n",
    "    Returns a list of (doc_id, score) tuples.\n",
    "    \"\"\"\n",
    "    return sorted(scores_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def query_scoring(query, total_number_of_docs, dictionary, k, champion_list=False):\n",
    "    # Initialize scores\n",
    "    cosine_scores = {}\n",
    "    jaccard_scores = {}\n",
    "\n",
    "    # Preprocess query\n",
    "    query_tokens = get_query_tokens(query)\n",
    "    query_tokens_count = get_query_tokens_count(query_tokens)\n",
    "    query_terms_num = sum(query_tokens_count.values())\n",
    "\n",
    "    print(query_tokens_count)\n",
    "\n",
    "    # Compute and update scores for each term in the query\n",
    "    for term in query_tokens_count:\n",
    "        w_tq, term_docs = compute_query_weight(term,\n",
    "                                               query_tokens_count,\n",
    "                                               dictionary,\n",
    "                                               champion_list,\n",
    "                                               total_number_of_docs)\n",
    "        if w_tq != 0:\n",
    "            update_doc_scores(term_docs, w_tq, cosine_scores, jaccard_scores)\n",
    "\n",
    "    # Finalize scores\n",
    "    finalize_cosine_scores(cosine_scores)\n",
    "    finalize_jaccard_scores(jaccard_scores, query_terms_num)\n",
    "\n",
    "    # Sort and retrieve top k results\n",
    "    sorted_doc_cosine = sort_scores(cosine_scores)\n",
    "    sorted_doc_jaccard = sort_scores(jaccard_scores)\n",
    "\n",
    "    return sorted_doc_cosine[:k], sorted_doc_jaccard[:k]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-10T20:44:51.910025Z",
     "start_time": "2025-01-10T20:44:51.855864Z"
    }
   },
   "id": "3ccece90c44cebf6"
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    dict_result = {}\n",
    "    print(\"---------- Results ----------\")\n",
    "\n",
    "    for rank, (doc_id, _) in enumerate(results, start=1):\n",
    "        if doc_id is None:\n",
    "            continue\n",
    "        \n",
    "        print(f\"Rank: {rank} | ID: {doc_id}\")\n",
    "        print(f\"Title : {docs[str(doc_id)]['title']}\")\n",
    "        print(f\"URL   : {docs[str(doc_id)]['url']}\")\n",
    "        print(\"-\" * 50)  \n",
    "\n",
    "        dict_result[rank] = {\n",
    "            'docID': doc_id,\n",
    "            'title': docs[str(doc_id)][\"title\"],\n",
    "            'url'  : docs[str(doc_id)][\"url\"]\n",
    "        }\n",
    "\n",
    "    return dict_result\n",
    "\n",
    "def query_search(query, result_numbers=5, champion_list=False):\n",
    "    results_cosine, results_jaccard = query_scoring(query, len(docs), dictionary, result_numbers, champion_list)\n",
    "    \n",
    "    if not results_cosine and not results_jaccard:\n",
    "        print(\"no results found\")\n",
    "        return {}, {}\n",
    "\n",
    "    print(\"=== Cosine Scores ===\")\n",
    "    r1 = print_results(results_cosine)\n",
    "\n",
    "    print(\"\\n=== Jaccard Scores ===\")\n",
    "    r2 = print_results(results_jaccard)\n",
    "\n",
    "    return r1, r2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-10T20:44:51.910388Z",
     "start_time": "2025-01-10T20:44:51.862940Z"
    }
   },
   "id": "41aa14fca3cebbff"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "87f780752010a316"
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'کریسمس': 1}\n",
      "=== Cosine Scores ===\n",
      "---------- Results ----------\n",
      "Rank: 1 | ID: 5933\n",
      "Title : ستاره اسپانیایی؛ هدیه کریسمس گواردیولا به ژاوی+عکس\n",
      "URL   : https://www.farsnews.ir/news/14001007000739/ستاره-اسپانیایی-هدیه-کریسمس-گواردیولا-به-ژاوی-عکس\n",
      "--------------------------------------------------\n",
      "Rank: 2 | ID: 6117\n",
      "Title : کی‌روش «دیکتاتور» لقب گرفت/اختلاف مرد پرتغالی با مصری‌ها به خاطر کریسمس+عکس\n",
      "URL   : https://www.farsnews.ir/news/14001005000165/کی‌روش-دیکتاتور-لقب-گرفت-اختلاف-مرد-پرتغالی-با-مصری‌ها-به-خاطر-کریسمس\n",
      "--------------------------------------------------\n",
      "Rank: 3 | ID: 6120\n",
      "Title : کشتار در ورزشگاه فوتبال در آستانه سال جدید\n",
      "URL   : https://www.farsnews.ir/news/14001005000143/کشتار-در-ورزشگاه-فوتبال-در-آستانه-سال-جدید\n",
      "--------------------------------------------------\n",
      "Rank: 4 | ID: 5926\n",
      "Title : مهاجم خارجی مس رفسنجان،4 کودک را به محل تحصیل برگرداند +عکس\n",
      "URL   : https://www.farsnews.ir/news/14001007000809/مهاجم-خارجی-مس-رفسنجان4-کودک-را-به-محل-تحصیل-برگرداند-عکس\n",
      "--------------------------------------------------\n",
      "Rank: 5 | ID: 5483\n",
      "Title : مسی چه زمانی به پاریس برمی گردد؟\n",
      "URL   : https://www.farsnews.ir/news/14001014000228/مسی-چه-زمانی-به-پاریس-برمی-گردد\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Jaccard Scores ===\n",
      "---------- Results ----------\n",
      "Rank: 1 | ID: 5933\n",
      "Title : ستاره اسپانیایی؛ هدیه کریسمس گواردیولا به ژاوی+عکس\n",
      "URL   : https://www.farsnews.ir/news/14001007000739/ستاره-اسپانیایی-هدیه-کریسمس-گواردیولا-به-ژاوی-عکس\n",
      "--------------------------------------------------\n",
      "Rank: 2 | ID: 5926\n",
      "Title : مهاجم خارجی مس رفسنجان،4 کودک را به محل تحصیل برگرداند +عکس\n",
      "URL   : https://www.farsnews.ir/news/14001007000809/مهاجم-خارجی-مس-رفسنجان4-کودک-را-به-محل-تحصیل-برگرداند-عکس\n",
      "--------------------------------------------------\n",
      "Rank: 3 | ID: 6120\n",
      "Title : کشتار در ورزشگاه فوتبال در آستانه سال جدید\n",
      "URL   : https://www.farsnews.ir/news/14001005000143/کشتار-در-ورزشگاه-فوتبال-در-آستانه-سال-جدید\n",
      "--------------------------------------------------\n",
      "Rank: 4 | ID: 5483\n",
      "Title : مسی چه زمانی به پاریس برمی گردد؟\n",
      "URL   : https://www.farsnews.ir/news/14001014000228/مسی-چه-زمانی-به-پاریس-برمی-گردد\n",
      "--------------------------------------------------\n",
      "Rank: 5 | ID: 5431\n",
      "Title : وقتی زیدان به رئال مادرید بازگشت+افتخارات\n",
      "URL   : https://www.farsnews.ir/news/14001014000983/وقتی-زیدان-به-رئال-مادرید-بازگشت-افتخارات\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "r1, r2 = query_search('کریسمس', result_numbers = 5, champion_list = True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-10T22:14:30.198955Z",
     "start_time": "2025-01-10T22:14:30.170731Z"
    }
   },
   "id": "cdd17c22afceb2ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1dd5f20cd6945f7c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
